{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write custom inference script and requirements to local folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir inference_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_code/inference.py\n",
    "\n",
    "# This is the script that will be used in the inference container\n",
    "import os \n",
    "import json \n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer for inference \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)\n",
    "    \n",
    "    model_dict = {'model':model, 'tokenizer':tokenizer}\n",
    "    \n",
    "    return model_dict \n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Make a prediction with the model\n",
    "    \"\"\"\n",
    "    text = input_data.pop('inputs')\n",
    "    parameters = input_data.pop('parameters', None)\n",
    "    \n",
    "    tokenizer = model['tokenizer']\n",
    "    model = model['model']\n",
    "\n",
    "    # Parameters may or may not be passed    \n",
    "    input_ids = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(input_ids, **parameters) if parameters is not None else model.generate(input_ids)\n",
    "    \n",
    "    return tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Transform the input request to a dictionary\n",
    "    \"\"\"\n",
    "    request = json.loads(request_body)\n",
    "\n",
    "    return request\n",
    "\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Return model's prediction\n",
    "    \"\"\"\n",
    "    return {'generated_text':prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_code/requirements.txt\n",
    "transformers\n",
    "sentencepiece\n",
    "protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy an endpoint with PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you .deploy(), this will upload your model package to S3, create a model in SageMaker, create an endpoint configuration, and deploy an endpoint from that configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "session_bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "pytorch_version = '1.7.1'\n",
    "python_version = 'py36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel \n",
    "\n",
    "model_name = 'summarization-model'\n",
    "endpoint_name = 'summarization-endpoint'\n",
    "\n",
    "model_for_deployment = HuggingFaceModel(entry_point='inference.py',\n",
    "                                        source_dir='inference_code',\n",
    "                                        model_data=huggingface_estimator.model_data,\n",
    "                                        # model_data=f'{session_bucket}/{<insert_model_location_key>}/model.tar.gz',            in case you don't run this notebook using the initialized huggingface_estimator from 2_finetune.ipynb\n",
    "                                        role=role,\n",
    "                                        pytorch_version=pytorch_version,\n",
    "                                        py_version=python_version,\n",
    "                                        transformers_version='4.6.1',\n",
    "                                        name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import BytesDeserializer\n",
    "\n",
    "# Deploy the model \n",
    "predictor = model_for_deployment.deploy(initial_instance_count=1,\n",
    "                                        instance_type='ml.m5.xlarge',\n",
    "                                        endpoint_name=endpoint_name\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.'\n",
    "        ' The aim is to reduce the risk of wildfires.' \n",
    "        'Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.'\n",
    ")\n",
    "\n",
    "summary_short = predictor.predict({\n",
    "    'inputs':text,\n",
    "    'parameters':{\n",
    "        'length_penalty':0.6\n",
    "    }\n",
    "})                                                              \n",
    "print(summary_short.decode())\n",
    "\n",
    "summary_long = predictor.predict({\n",
    "    'inputs':text,\n",
    "    'parameters':{\n",
    "        'length_penalty':1.5\n",
    "    }\n",
    "})      \n",
    "print(summary_long.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) If you haven't fine-tuned a model, but want to deploy directly from HuggingFace Hub to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pass these as env variables, defining the model and task we want \n",
    "hub = {\n",
    "  'HF_MODEL_ID':'google/pegasus-xsum',\n",
    "  'HF_TASK':'summarization' \n",
    "}\n",
    "\n",
    "hub_model = HuggingFaceModel(env=hub,\n",
    "                             role=role,\n",
    "                             pytorch_version='1.7',\n",
    "                             py_version='py36',\n",
    "                             transformers_version='4.6',\n",
    "                             name='hub-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_predictor = hub_model.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m5.xlarge',\n",
    "                                 endpoint_name='hub-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also pass in a 'parameters' key with valid parameters, just like we did before\n",
    "summary = hub_predictor.predict({'inputs':text}) \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code to delete the resources created in SageMaker InferenceÂ (endpoint configuration, endpoint and model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}